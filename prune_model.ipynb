{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from resnet import resnet56\n",
    "from embedl.plumbing.torch.metrics.target import Target\n",
    "from embedl.torch.pruning.methods import UniformPruning\n",
    "from embedl.torch.metrics.performances import Flops  \n",
    "from embedl.torch.metrics.measure_performance import measure_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| [04:16<00:00, 320.08s/it] (Flops) \n",
      "WARNING:embedl.plumbing.torch.core.module_processing:Found nodes that do not have (<class 'embedl.plumbing.torch.pruning.node_configs.base.DependencyConfig'>,) implemented: \n",
      "WARNING:embedl.plumbing.torch.core.module_processing:\n",
      "WARNING:embedl.plumbing.torch.core.module_processing:    0. <built-in function pad> (2 occurrences)\n",
      "WARNING:embedl.plumbing.torch.core.module_processing:\n",
      "WARNING:embedl.plumbing.torch.core.module_processing:This may limit certain functionality. Extend the dict returned by DependencyTracking.node_config_map() to extend support to these.\n",
      "WARNING:embedl.plumbing.torch.core.module_processing:Found nodes that do not have (<class 'embedl.plumbing.torch.pruning.node_configs.base.MaskChannelPruningConfig'>,) implemented: \n",
      "WARNING:embedl.plumbing.torch.core.module_processing:\n",
      "WARNING:embedl.plumbing.torch.core.module_processing:    0. input node (1 occurrence)\n",
      "WARNING:embedl.plumbing.torch.core.module_processing:    1. <function relu at 0x7f101e12b760> (55 occurrences)\n",
      "WARNING:embedl.plumbing.torch.core.module_processing:    2. <built-in function add> (27 occurrences)\n",
      "WARNING:embedl.plumbing.torch.core.module_processing:    3. <built-in function getitem> (2 occurrences)\n",
      "WARNING:embedl.plumbing.torch.core.module_processing:    4. <built-in function pad> (2 occurrences)\n",
      "WARNING:embedl.plumbing.torch.core.module_processing:    5. <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'> (1 occurrence)\n",
      "WARNING:embedl.plumbing.torch.core.module_processing:    6. <built-in method flatten of type object at 0x7f104a7fd460> (1 occurrence)\n",
      "WARNING:embedl.plumbing.torch.core.module_processing:    7. output node (1 occurrence)\n",
      "WARNING:embedl.plumbing.torch.core.module_processing:\n",
      "WARNING:embedl.plumbing.torch.core.module_processing:This may limit certain functionality. Extend the dict returned by MaskChannelPruning.node_config_map() to extend support to these.\n",
      "100%|█████████▉| [01:51<00:00, 104.45s/it] (Flops) "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19897505715220812\n"
     ]
    }
   ],
   "source": [
    "model = resnet56()\n",
    "input_shape = [1,3,32,32]       \n",
    "base_flops = measure_flops(\n",
    "    model=model,\n",
    "    input_shape=input_shape\n",
    ")\n",
    "pruning_method = UniformPruning(\n",
    "    target=Target(Flops(), fraction=0.2),\n",
    "    step_size=1\n",
    ")\n",
    "pruning_steps = pruning_method.prune(model, input_shape)\n",
    "pruned_flops = measure_flops(\n",
    "    model=model,\n",
    "    input_shape=input_shape\n",
    ")\n",
    "print(pruned_flops/base_flops) # 0.19897505715220812\n",
    "torch.save(model, 'resnet56_pruned/resnet56_uniform_20.th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_embedl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
