{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from models import mobilenetv2, resnet56\n",
    "from torchvision.models import resnet50\n",
    "from embedl.plumbing.torch.metrics.target import Target\n",
    "from embedl.torch.pruning.methods import UniformPruning\n",
    "from embedl.torch.viewer import view_model\n",
    "from embedl.torch.metrics.performances import Flops  \n",
    "from embedl.torch.metrics.measure_performance import measure_flops\n",
    "import torchvision.datasets as datasets\n",
    "from embedl.torch.pruning.methods import plot_pruning_profile \n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from embedl.torch.metrics.performances import Flops\n",
    "from embedl.torch.pruning.methods import (\n",
    "    PruningMethod,\n",
    "    ChannelPruningTactic,\n",
    ")\n",
    "from embedl.plumbing.torch.metrics.scorers import ChannelPruningScorer, PruningBalancer\n",
    "from embedl.torch.metrics.importance_scores import WeightMagnitude\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261]\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(\n",
    "        root=\"/home/jonna/data\",\n",
    "        train=False,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    \"\"\"\n",
    "    Run evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    prec1 = 0\n",
    "    count = 0\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            target = target.cuda()\n",
    "            input_var = input.cuda()\n",
    "            target_var = target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input_var)\n",
    "            loss = criterion(output, target_var)\n",
    "\n",
    "            output = output.float()\n",
    "            loss = loss.float()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 += accuracy(output.data, target)[0] * target.size(0)\n",
    "            # print(accuracy(output.data, target)[0])\n",
    "            count += target.size(0)\n",
    "\n",
    "    print(f\" * Prec@1 {prec1/count:.3f}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('/home/jonna/hyperparameter_sensitivity_pruning/experiments/imagenet/base_model/results/lr_10**-1.00_wd_10**-4.00/checkpoint.pth')\n",
    "model['model']\n",
    "r = resnet50()\n",
    "r.load_state_dict(model['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found nodes that do not have (<class 'embedl.plumbing.torch.pruning.node_configs.base.MaskChannelPruningConfig'>,) implemented: \n",
      "\n",
      "    0. input node (1 occurrence)\n",
      "    1. <class 'torch.nn.modules.activation.ReLU'> (49 occurrences)\n",
      "    2. <class 'torch.nn.modules.pooling.MaxPool2d'> (1 occurrence)\n",
      "    3. <built-in function add> (16 occurrences)\n",
      "    4. <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'> (1 occurrence)\n",
      "    5. <built-in method flatten of type object at 0x7f3e715fd460> (1 occurrence)\n",
      "    6. output node (1 occurrence)\n",
      "\n",
      "This may limit certain functionality. Extend the dict returned by MaskChannelPruning.node_config_map() to extend support to these.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8242043856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | [01:20<12:27:05, 74816.33s/it] (Flops)  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(measure_flops(model\u001b[39m=\u001b[39mr, input_shape\u001b[39m=\u001b[39minput_shape)\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m pruning_method \u001b[39m=\u001b[39m UniformPruning(target\u001b[39m=\u001b[39mTarget(Flops(), fraction\u001b[39m=\u001b[39m\u001b[39m0.4\u001b[39m), step_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m pruning_steps \u001b[39m=\u001b[39m pruning_method\u001b[39m.\u001b[39;49mprune(r, input_shape)\n\u001b[1;32m      6\u001b[0m pruned_flops \u001b[39m=\u001b[39m measure_flops(model\u001b[39m=\u001b[39mr, input_shape\u001b[39m=\u001b[39minput_shape)\n",
      "File \u001b[0;32m~/venv_embedl/lib/python3.10/site-packages/embedl/plumbing/torch/pruning/method.py:436\u001b[0m, in \u001b[0;36mPruningMethod.prune\u001b[0;34m(self, model, input_shape, step_callback, keep_masks, optimizer, visualize)\u001b[0m\n\u001b[1;32m    433\u001b[0m steps\u001b[39m.\u001b[39mappend(initial_step)\n\u001b[1;32m    435\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target_reached():\n\u001b[0;32m--> 436\u001b[0m     step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_step(embedl_graph_module)\n\u001b[1;32m    437\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m step:\n\u001b[1;32m    438\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    439\u001b[0m             _get_target_not_reached_warning(\n\u001b[1;32m    440\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tracker\n\u001b[1;32m    441\u001b[0m             )\n\u001b[1;32m    442\u001b[0m         )\n",
      "File \u001b[0;32m~/venv_embedl/lib/python3.10/site-packages/embedl/plumbing/torch/pruning/method.py:239\u001b[0m, in \u001b[0;36mPruningMethod._next_step\u001b[0;34m(self, embedl_graph_module)\u001b[0m\n\u001b[1;32m    237\u001b[0m candidates \u001b[39m=\u001b[39m []\n\u001b[1;32m    238\u001b[0m \u001b[39mfor\u001b[39;00m tactic \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtactics:\n\u001b[0;32m--> 239\u001b[0m     step \u001b[39m=\u001b[39m tactic\u001b[39m.\u001b[39;49mgenerate_candidate_step(\n\u001b[1;32m    240\u001b[0m         embedl_graph_module,\n\u001b[1;32m    241\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscorer,\n\u001b[1;32m    242\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tracker,\n\u001b[1;32m    243\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_include_ops,\n\u001b[1;32m    244\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_exclude_ops,\n\u001b[1;32m    245\u001b[0m     )\n\u001b[1;32m    246\u001b[0m     \u001b[39mif\u001b[39;00m step:\n\u001b[1;32m    247\u001b[0m         candidates\u001b[39m.\u001b[39mappend(step)\n",
      "File \u001b[0;32m~/venv_embedl/lib/python3.10/site-packages/embedl/plumbing/torch/pruning/tactics.py:364\u001b[0m, in \u001b[0;36mChannelDependencyGroupTactic.generate_candidate_step\u001b[0;34m(self, embedl_graph_module, scorer, tracker, include_ops, exclude_ops)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    361\u001b[0m backed_up_masks \u001b[39m=\u001b[39m MaskChannelPruning\u001b[39m.\u001b[39msave_mask_dict(\n\u001b[1;32m    362\u001b[0m     embedl_graph_module, group\u001b[39m.\u001b[39mnodes\n\u001b[1;32m    363\u001b[0m )\n\u001b[0;32m--> 364\u001b[0m candidate_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcandidate_step_for_group(\n\u001b[1;32m    365\u001b[0m     group, embedl_graph_module, scorer, tracker\n\u001b[1;32m    366\u001b[0m )\n\u001b[1;32m    367\u001b[0m \u001b[39mif\u001b[39;00m candidate_step:\n\u001b[1;32m    368\u001b[0m     \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m group\u001b[39m.\u001b[39mnodes:\n",
      "File \u001b[0;32m~/venv_embedl/lib/python3.10/site-packages/embedl/plumbing/torch/pruning/tactics.py:466\u001b[0m, in \u001b[0;36mChannelPruningTactic.candidate_step_for_group\u001b[0;34m(self, group, embedl_graph_module, scorer, tracker)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcandidate_step_for_group\u001b[39m(\n\u001b[1;32m    457\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    458\u001b[0m     group: ChannelDependencyGroup,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    464\u001b[0m \n\u001b[1;32m    465\u001b[0m     \u001b[39m# Not aggregating scores will speedup the pruning.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m     channel_queue \u001b[39m=\u001b[39m get_sorted_channel_indices(\n\u001b[1;32m    467\u001b[0m         embedl_graph_module,\n\u001b[1;32m    468\u001b[0m         group,\n\u001b[1;32m    469\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchannel_selection_quantity,\n\u001b[1;32m    470\u001b[0m         aggregate_scores\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_speedup_pruning,\n\u001b[1;32m    471\u001b[0m     )\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m    473\u001b[0m     \u001b[39m# This method assumes that the channel indices are ordered in such a way that\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[39m# all channels on the main edge that are dependent on each other come in one\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[39m# sequence before the next group of channels that are dependent on each other.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[39m# This currently holds true when all channels have unique attribute values.\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     \u001b[39m# TODO: Make sure this holds true even in cases where all channels have the same value.\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     previous_channels \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/venv_embedl/lib/python3.10/site-packages/embedl/plumbing/torch/pruning/prune.py:318\u001b[0m, in \u001b[0;36mget_sorted_channel_indices\u001b[0;34m(embedl_graph_module, dependency_group, channel_quantity, relations, aggregate_scores, exclude_masked_indices)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_sorted_channel_indices\u001b[39m(\n\u001b[1;32m    279\u001b[0m     embedl_graph_module: EmbedlGraphModule,\n\u001b[1;32m    280\u001b[0m     dependency_group: Union[ChannelDependencyGroup, \u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    289\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    290\u001b[0m     \u001b[39m# pylint: disable=too-many-arguments\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m    Return a 1D tensor of channel indices sorted in ascending order by `channel_quantity`.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39m        A 1D tensor with one score for each channel on the main edge of the dependency group.\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     sorted_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margsort(\n\u001b[0;32m--> 318\u001b[0m         get_attribute_scores(\n\u001b[1;32m    319\u001b[0m             embedl_graph_module,\n\u001b[1;32m    320\u001b[0m             dependency_group,\n\u001b[1;32m    321\u001b[0m             channel_quantity,\n\u001b[1;32m    322\u001b[0m             relations,\n\u001b[1;32m    323\u001b[0m             aggregate_scores\u001b[39m=\u001b[39;49maggregate_scores,\n\u001b[1;32m    324\u001b[0m         )\n\u001b[1;32m    325\u001b[0m     )\n\u001b[1;32m    327\u001b[0m     \u001b[39mif\u001b[39;00m exclude_masked_indices:\n\u001b[1;32m    328\u001b[0m         node_config \u001b[39m=\u001b[39m MaskChannelPruning\u001b[39m.\u001b[39mconfig(dependency_group\u001b[39m.\u001b[39mmain_node)\n",
      "File \u001b[0;32m~/venv_embedl/lib/python3.10/site-packages/embedl/plumbing/torch/pruning/prune.py:266\u001b[0m, in \u001b[0;36mget_attribute_scores\u001b[0;34m(embedl_graph_module, dependency_group, channel_quantity, relations, aggregate_scores)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(current_scores) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    264\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    265\u001b[0m         projected_scores \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 266\u001b[0m             DependencyTracking\u001b[39m.\u001b[39;49mchannel_wise_attribute_on_edge(\n\u001b[1;32m    267\u001b[0m                 dependency_group,\n\u001b[1;32m    268\u001b[0m                 current_scores,\n\u001b[1;32m    269\u001b[0m                 from_edge\u001b[39m=\u001b[39;49medge,\n\u001b[1;32m    270\u001b[0m                 to_edge\u001b[39m=\u001b[39;49mdependency_group\u001b[39m.\u001b[39;49mmain_edge,\n\u001b[1;32m    271\u001b[0m             )\n\u001b[1;32m    272\u001b[0m         )\n\u001b[1;32m    273\u001b[0m         scores \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m projected_scores\n\u001b[1;32m    275\u001b[0m \u001b[39mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/venv_embedl/lib/python3.10/site-packages/embedl/plumbing/torch/pruning/dependency_tracking.py:597\u001b[0m, in \u001b[0;36mDependencyTracking.channel_wise_attribute_on_edge\u001b[0;34m(cls, channel_dependency_group, attribute_tensor_from_edge, from_edge, to_edge)\u001b[0m\n\u001b[1;32m    593\u001b[0m channel_axis \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mget_channel_axis_on_edge(to_edge)\n\u001b[1;32m    594\u001b[0m indices_on_to \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\n\u001b[1;32m    595\u001b[0m     start\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, end\u001b[39m=\u001b[39mto_edge\u001b[39m.\u001b[39mshape[channel_axis], step\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    596\u001b[0m )\u001b[39m.\u001b[39mlong()\n\u001b[0;32m--> 597\u001b[0m tensor_dependencies \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mtensor_dependencies_from_channel_indices(\n\u001b[1;32m    598\u001b[0m     indices\u001b[39m=\u001b[39;49mindices_on_to,\n\u001b[1;32m    599\u001b[0m     from_edge\u001b[39m=\u001b[39;49mto_edge,\n\u001b[1;32m    600\u001b[0m     get_index_by_batch\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    601\u001b[0m )\n\u001b[1;32m    603\u001b[0m \u001b[39mif\u001b[39;00m from_edge \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m tensor_dependencies:\n\u001b[1;32m    604\u001b[0m     \u001b[39m# If there is no dependency between the two edges, return a zero vector.\u001b[39;00m\n\u001b[1;32m    605\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mzeros(to_edge\u001b[39m.\u001b[39mshape[channel_axis])\n",
      "File \u001b[0;32m~/venv_embedl/lib/python3.10/site-packages/embedl/plumbing/torch/pruning/dependency_tracking.py:277\u001b[0m, in \u001b[0;36mDependencyTracking.tensor_dependencies_from_channel_indices\u001b[0;34m(cls, indices, from_edge, get_index_by_batch)\u001b[0m\n\u001b[1;32m    272\u001b[0m tensor_dependencies \u001b[39m=\u001b[39m {from_edge: tensor_dependency}\n\u001b[1;32m    273\u001b[0m tensor_dependencies \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mrecursively_trace_tensor_dependencies(\n\u001b[1;32m    274\u001b[0m     DirectedEdge(from_node_2, from_node_1),\n\u001b[1;32m    275\u001b[0m     tensor_dependencies,\n\u001b[1;32m    276\u001b[0m )\n\u001b[0;32m--> 277\u001b[0m tensor_dependencies \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mrecursively_trace_tensor_dependencies(\n\u001b[1;32m    278\u001b[0m     DirectedEdge(from_node_1, from_node_2),\n\u001b[1;32m    279\u001b[0m     tensor_dependencies,\n\u001b[1;32m    280\u001b[0m )\n\u001b[1;32m    282\u001b[0m \u001b[39mreturn\u001b[39;00m tensor_dependencies\n",
      "File \u001b[0;32m~/venv_embedl/lib/python3.10/site-packages/embedl/plumbing/torch/pruning/dependency_tracking.py:370\u001b[0m, in \u001b[0;36mDependencyTracking.recursively_trace_tensor_dependencies\u001b[0;34m(cls, next_edge, tensor_dependencies)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    368\u001b[0m         tensor_dependencies[neighbor_edge] \u001b[39m=\u001b[39m neighbor_tensor_dependency\n\u001b[0;32m--> 370\u001b[0m     tensor_dependencies \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mrecursively_trace_tensor_dependencies(\n\u001b[1;32m    371\u001b[0m         DirectedEdge(node, neighbor_node),\n\u001b[1;32m    372\u001b[0m         tensor_dependencies,\n\u001b[1;32m    373\u001b[0m     )\n\u001b[1;32m    375\u001b[0m \u001b[39mreturn\u001b[39;00m tensor_dependencies\n",
      "File \u001b[0;32m~/venv_embedl/lib/python3.10/site-packages/embedl/plumbing/torch/pruning/dependency_tracking.py:370\u001b[0m, in \u001b[0;36mDependencyTracking.recursively_trace_tensor_dependencies\u001b[0;34m(cls, next_edge, tensor_dependencies)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    368\u001b[0m         tensor_dependencies[neighbor_edge] \u001b[39m=\u001b[39m neighbor_tensor_dependency\n\u001b[0;32m--> 370\u001b[0m     tensor_dependencies \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mrecursively_trace_tensor_dependencies(\n\u001b[1;32m    371\u001b[0m         DirectedEdge(node, neighbor_node),\n\u001b[1;32m    372\u001b[0m         tensor_dependencies,\n\u001b[1;32m    373\u001b[0m     )\n\u001b[1;32m    375\u001b[0m \u001b[39mreturn\u001b[39;00m tensor_dependencies\n",
      "    \u001b[0;31m[... skipping similar frames: DependencyTracking.recursively_trace_tensor_dependencies at line 370 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/venv_embedl/lib/python3.10/site-packages/embedl/plumbing/torch/pruning/dependency_tracking.py:370\u001b[0m, in \u001b[0;36mDependencyTracking.recursively_trace_tensor_dependencies\u001b[0;34m(cls, next_edge, tensor_dependencies)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    368\u001b[0m         tensor_dependencies[neighbor_edge] \u001b[39m=\u001b[39m neighbor_tensor_dependency\n\u001b[0;32m--> 370\u001b[0m     tensor_dependencies \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mrecursively_trace_tensor_dependencies(\n\u001b[1;32m    371\u001b[0m         DirectedEdge(node, neighbor_node),\n\u001b[1;32m    372\u001b[0m         tensor_dependencies,\n\u001b[1;32m    373\u001b[0m     )\n\u001b[1;32m    375\u001b[0m \u001b[39mreturn\u001b[39;00m tensor_dependencies\n",
      "File \u001b[0;32m~/venv_embedl/lib/python3.10/site-packages/embedl/plumbing/torch/pruning/dependency_tracking.py:342\u001b[0m, in \u001b[0;36mDependencyTracking.recursively_trace_tensor_dependencies\u001b[0;34m(cls, next_edge, tensor_dependencies)\u001b[0m\n\u001b[1;32m    331\u001b[0m neighbor_tensor_dependencies \u001b[39m=\u001b[39m node_config\u001b[39m.\u001b[39mtensor_dependency(\n\u001b[1;32m    332\u001b[0m     node,\n\u001b[1;32m    333\u001b[0m     prev_node,\n\u001b[1;32m    334\u001b[0m     tensor_dependencies[Edge(\u001b[39m*\u001b[39mnext_edge)],\n\u001b[1;32m    335\u001b[0m )\n\u001b[1;32m    337\u001b[0m \u001b[39mfor\u001b[39;00m (\n\u001b[1;32m    338\u001b[0m     neighbor_node,\n\u001b[1;32m    339\u001b[0m     neighbor_tensor_dependency,\n\u001b[1;32m    340\u001b[0m ) \u001b[39min\u001b[39;00m neighbor_tensor_dependencies\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    341\u001b[0m     \u001b[39m# Don't look further when dependencies are broken.\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39;49mcount_nonzero(neighbor_tensor_dependency) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    343\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     check_consistent_tensor_dependency_shape(\n\u001b[1;32m    346\u001b[0m         embedl_graph_module,\n\u001b[1;32m    347\u001b[0m         node,\n\u001b[1;32m    348\u001b[0m         neighbor_node,\n\u001b[1;32m    349\u001b[0m         neighbor_tensor_dependency,\n\u001b[1;32m    350\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_shape = [1, 3, 224, 224]\n",
    "print(measure_flops(model=r, input_shape=input_shape)\n",
    ")\n",
    "pruning_method = UniformPruning(target=Target(Flops(), fraction=0.4), step_size=1)\n",
    "pruning_steps = pruning_method.prune(r, input_shape)\n",
    "pruned_flops = measure_flops(model=r, input_shape=input_shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_embedl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
