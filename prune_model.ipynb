{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from models import mobilenetv2, resnet56\n",
    "from embedl.plumbing.torch.metrics.target import Target\n",
    "from embedl.torch.pruning.methods import UniformPruning\n",
    "from embedl.torch.viewer import view_model\n",
    "from embedl.torch.metrics.performances import Flops  \n",
    "from embedl.torch.metrics.measure_performance import measure_flops\n",
    "import torchvision.datasets as datasets\n",
    "from embedl.torch.pruning.methods import plot_pruning_profile \n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from embedl.torch.metrics.performances import Flops\n",
    "from embedl.torch.pruning.methods import (\n",
    "    PruningMethod,\n",
    "    ChannelPruningTactic,\n",
    ")\n",
    "from embedl.plumbing.torch.metrics.scorers import ChannelPruningScorer, PruningBalancer\n",
    "from embedl.torch.metrics.importance_scores import WeightMagnitude\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261]\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(\n",
    "        root=\"/home/jonna/data\",\n",
    "        train=False,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    \"\"\"\n",
    "    Run evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    prec1 = 0\n",
    "    count = 0\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            target = target.cuda()\n",
    "            input_var = input.cuda()\n",
    "            target_var = target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input_var)\n",
    "            loss = criterion(output, target_var)\n",
    "\n",
    "            output = output.float()\n",
    "            loss = loss.float()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 += accuracy(output.data, target)[0] * target.size(0)\n",
    "            # print(accuracy(output.data, target)[0])\n",
    "            count += target.size(0)\n",
    "\n",
    "    print(f\" * Prec@1 {prec1/count:.3f}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\n",
    "    \"/home/jonna/hyperparameter_sensitivity_pruning/experiments/cifar10/mobilenetv2/base_model/results_0/lr_10**-1.00_wd_10**-4.00/checkpoint_final.th\"\n",
    ")[\"state_dict\"]\n",
    "state_dict = {key[7:]: weights for key, weights in state_dict.items()}\n",
    "model = mobilenetv2()\n",
    "model.load_state_dict(state_dict)\n",
    "model.cuda()\n",
    "\n",
    "print(\"Validate before pruning\")\n",
    "validate(val_loader, torch.nn.DataParallel(model), nn.CrossEntropyLoss().cuda())\n",
    "\n",
    "input_shape = [1, 3, 32, 32]\n",
    "base_flops = measure_flops(model=model, input_shape=input_shape)\n",
    "\n",
    "\n",
    "scorer = ChannelPruningScorer(\n",
    "    importance_score=WeightMagnitude(), channel_pruning_balancer=None\n",
    ")\n",
    "tactic = ChannelPruningTactic(step_size=1, search_depth=1, speedup_pruning=False)\n",
    "\n",
    "pruning_method = PruningMethod(scorer, [tactic], target=Target(Flops(), fraction=0.6))\n",
    "pruning_steps = pruning_method.prune(model, input_shape=input_shape)\n",
    "\n",
    "print(\"\\nValidate after pruning\")\n",
    "validate(val_loader, torch.nn.DataParallel(model), nn.CrossEntropyLoss().cuda())\n",
    "\n",
    "pruned_flops = measure_flops(model=model, input_shape=input_shape)\n",
    "print(pruned_flops / base_flops)  # 0.5999830832084312\n",
    "\n",
    "plot_pruning_profile(model, pruning_steps)\n",
    "torch.save(model, 'pruned_models/mobilenetv2_magnitude_60.th')\n",
    "with open('pruned_models/pruningsteps_mobilenetv2_magnitude_60.th', 'wb') as f:\n",
    "    pickle.dump(pruning_steps, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_embedl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
